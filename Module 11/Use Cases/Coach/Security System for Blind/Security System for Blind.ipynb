{"metadata":{"colab":{"collapsed_sections":[],"name":"Security System for Blind.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome to 'Security system for the blind'\n\nCurrently, it is difficult for blind people to tell who is at their door without the help of an external system. They are capable of hearing but with voice recording and maniupulation there is a window of opportunity for people to take advantage of them. We will be using face recognition with a security camera to detect people and try to identify their faces. We will then be using the 'pyttsx3' to convert the name of the person detected to speech for the blind person. ","metadata":{"id":"60E4QunViemt"}},{"cell_type":"markdown","source":"In order to detect the faces from the security camera we will be using the 'face-detection-adas', 'face-reidentification-retail-0095', 'landmarks-regression-retail-0009' pre-trained models from the OpenVINO model zoo. You can find out more about it here ->https://docs.openvinotoolkit.org/2019_R1/usergroup1.html\n","metadata":{"id":"M8vvCEiJiemu"}},{"cell_type":"code","source":"pip install google.colab","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from goolge.colab.patches import cv2_imshow","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom utils.opv import OpvModel\nfrom sklearn.metrics.pairwise import cosine_similarity","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:21:54.109846Z","start_time":"2021-03-19T07:21:52.403351Z"},"id":"845uefAViemv","outputId":"54bd2834-132b-4eff-ecdd-2c8de39c888c"},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Task: Load the pretrained model ","metadata":{"id":"vDPYa2sSiem2"}},{"cell_type":"code","source":"mymodel0 = OpvModel(\"face-detection-adas-0001\", device=\"CPU\", fp=\"FP16\")\nmymodel1 = OpvModel(\"landmarks-regression-retail-0009\", device=\"CPU\", fp=\"FP32\")\nmymodel2 = OpvModel(\"face-reidentification-retail-0095\", device=\"CPU\", fp=\"FP32\")","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:21:57.968238Z","start_time":"2021-03-19T07:21:56.518530Z"},"id":"pwczAr2Oiem2"},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Task: Draw Bounding Box around the Face\n\nWe create a function to draw a bounding box around the face image that has been detected by face-detection-adas-0001.\n\nAdditionally, we will store the captured image of a face into the variable face_box for further processing.","metadata":{"id":"994J7DV4ienC"}},{"cell_type":"code","source":"def DrawBoundingBoxes(predictions, image, conf=.2):\n    canvas = image.copy()                             # copy instead of modifying the original image\n    predictions_1 = predictions[0][0]                 # subset dataframe\n    confidence = predictions_1[:,2]                   # getting conf value [image_id, label, conf, x_min, y_min, x_max, y_max]\n    topresults = predictions_1[(confidence>conf)]     # choosing only predictions with conf value bigger than treshold\n    (h,w) = canvas.shape[:2]                        # \n    for detection in topresults:\n        \n        box = detection[3:7] * np.array([w, h, w, h]) # determine box location\n        (xmin, ymin, xmax, ymax) = box.astype(\"int\") # assign box location value to xmin, ymin, xmax, ymax\n\n        cv2.rectangle(canvas, (xmin, ymin), (xmax, ymax), (0, 0, 255), 4)  # make a rectangle\n        cv2.putText(canvas, str(round(detection[2]*100,1))+\"%\", (xmin, ymin), # include text\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n        cv2.putText(canvas, str(len(topresults))+\" face(s) detected\", (50,50), # include text\n            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0,0), 2)\n    \n    global face_box\n    face_box = canvas[ymin:ymax,xmin:xmax]\n    \n    cv2_imshow(face_box)\n             \n    #Show result image for 0,5 sec on screen\n    return ","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:22:03.449872Z","start_time":"2021-03-19T07:22:03.437909Z"},"id":"hYWRtHvdienD"},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Task: Create a Function to Warp Photo Given the Reference Points\n\nHere, we create a function that does several things:\n- find out the 5 landmarks within the face image: left and right eyes, nose, left and right corner of lips\n- using perspective transformation to warp the photo into the format required by the face reidentification model\n- store the new image in the variable called dst","metadata":{"id":"Cm3mlPXqienO"}},{"cell_type":"code","source":"def WarpPhoto(src):\n    predictions = mymodel1.Predict(src)\n    \n    DIM = src.shape[0] \n    \n    l_eye = tuple([int(DIM*i) for i in (predictions[0,0,0,0], predictions[0,1,0,0])])   \n    r_eye = tuple([int(DIM*i) for i in (predictions[0,2,0,0], predictions[0,3,0,0])]) \n    nose = tuple([int(DIM*i) for i in (predictions[0,4,0,0], predictions[0,5,0,0])]) \n    l_lip = tuple([int(DIM*i) for i in (predictions[0,6,0,0], predictions[0,7,0,0])]) \n    r_lip = tuple([int(DIM*i) for i in (predictions[0,8,0,0], predictions[0,9,0,0])]) \n    \n    pts1 = np.float32([l_eye,r_eye,l_lip,r_lip])\n    pts2 = np.float32([[DIM*0.31556875000000000, DIM*0.4615741071428571],\\\n                   [DIM*0.68262291666666670, DIM*0.4615741071428571],\\\n                   [DIM*0.34947187500000004, DIM*0.8246919642857142],\\\n                   [DIM*0.65343645833333330, DIM*0.8246919642857142]])\n\n    M = cv2.getPerspectiveTransform(pts1,pts2)\n    \n    global dst\n    dst = cv2.warpPerspective(src,M, (DIM,DIM))\n    \n    cv2.imshow('Warp', dst)\n    cv2.waitKey(10)\n    return   ","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:22:07.655628Z","start_time":"2021-03-19T07:22:07.640594Z"},"id":"pZKOuz2-ienP"},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Task: Get Face Vector from Warped Photos\n\nOnce we have the image processed to the format we want, we will now use the face-reidentification model to create a vector corresponding to the processed image. The idea is that every face will have different vectors. ","metadata":{"id":"9c1gBQJfiene"}},{"cell_type":"code","source":"def GetVec(img):\n    prediction1 = mymodel2.Predict(img)\n    \n    global prediction1_flat\n    prediction1_flat = prediction1.flatten()\n    prediction1_flat = prediction1_flat.reshape(1, 256)\n    \n    print (prediction1_flat)\n    \n    return prediction1_flat","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:22:10.379105Z","start_time":"2021-03-19T07:22:10.372104Z"},"id":"Ia4vYp-aienf"},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Task: Import Vector Database\n\nHere, we import our vector database that contains the vector set of person's photos","metadata":{"id":"I97soUkcienw"}},{"cell_type":"code","source":"import pickle\nwith open('./resources/database/arihant.pickle', 'rb') as f:\n    predictiona1_flat = pickle.load(f)\n    \nwith open('./resources/database/daniel.pickle', 'rb') as f:\n    predictiond1_flat = pickle.load(f)\n    \nwith open('./resources/database/arvin.pickle', 'rb') as f:\n    predictionav1_flat = pickle.load(f)","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:22:13.156320Z","start_time":"2021-03-19T07:22:13.149300Z"},"id":"lpGHWlf8ienx"},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Task: Compare Vector with Database and output the name in a sound format.","metadata":{"id":"FxdG1TnOien9"}},{"cell_type":"code","source":"import pyttsx3\nengine = pyttsx3.init()\n\n\ndef cosim(vec):\n    sim = cosine_similarity(vec, predictiond1_flat)\n    \n    if cosine_similarity(vec, predictiond1_flat) > 0.5: #if cosine similarity with daniel is high\n        friends = cv2.imread(\"./resources/images/Daniel/Daniel.jpeg\")\n        cv2.imshow('It is Daniel', friends)\n        engine.say(\"It is Daniel\")  # using the engine here\n        engine.runAndWait()\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n        \n    elif cosine_similarity(vec, predictiona1_flat) > 0.5: #if cosine similarity with Arihant is high\n        friends = cv2.imread(\"./resources/images/Arihant/Arihant.jpeg\")\n        cv2.imshow('It is Arihant', friends)\n        engine.say(\"It is Arihant\")\n        engine.runAndWait()\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n    \n    elif cosine_similarity(vec, predictionav1_flat) > 0.5: #if cosine similarity with Arihant is high\n        friends = cv2.imread(\"./resources/images/Arvin/Arvin.jpg\")\n        cv2.imshow('It is Arvin', friends)\n        engine.say(\"It is Arvin\")\n        engine.runAndWait()\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n        \n    else : #if cosine similarity with daniel is high\n        print(\"Do you belong here?\")\n        friends = cv2.imread(\"./resources/images/denied.jpeg\")\n        cv2.imshow('Do you belong here?', friends)\n        engine.say(\"Not found\")   # using the engine here\n        engine.runAndWait() \n        cv2.waitKey(0)\n        cv2.destroyAllWindows()","metadata":{"ExecuteTime":{"end_time":"2021-03-19T07:22:16.589216Z","start_time":"2021-03-19T07:22:16.390145Z"},"id":"4_-Xcpxnien_"},"execution_count":7,"outputs":[]}]}